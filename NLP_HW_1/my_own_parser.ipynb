{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание\n",
        "Откуда берутся датасеты\n",
        "\n",
        "### Цель:\n",
        "В этом ДЗ вы напишите свой парсер, который будет бегать по страничкам и автоматически что-то собирать.\n",
        "\n",
        "\n",
        "### Описание/Пошаговая инструкция выполнения домашнего задания:\n",
        "По аналогии с занятием по парсингу данных, возьмите интересующий вас сайт, на котором можно пособирать какие-то данные (и при этом API не предоставляется).\n",
        "\n",
        "Идеальный датасет должен иметь текстовое описание некоторого объекта и некоторую целевую переменную, соответствующую этому объекту. Например:\n",
        "- Сайт новостей: текстовое описание - сама новость, целевая переменная - количество просмотров новости (можно поделить на число дней с момента даты публикации, чтобы получить “среднее число просмотров в день”).\n",
        "- Сайт с товарами/книгами/фильмами: текстовое описание товара/книги/фильма + средний рейтинг в качестве целевой переменной.\n",
        "- Блоги - тексты заметок + число просмотров.\n",
        "- И любые другие ваши идеи, которые подходят под такой формат.\n",
        "\n",
        "Напишите свой парсер, который будет бегать по страничкам и автоматически что-то собирать.\n",
        "Не забывайте, что парсинг - это ответственное мероприятие, поэтому не бомбардируйте несчастные сайты слишком частыми запросами (можно ограничить число запросов в секунду при помощи time.sleep(0.3), вставленного в теле цикла)\n",
        "\n",
        "При необходимости очистить датасет от мусора с помощью регулярных выражений).\n",
        "\n",
        "Посчитать статистики по собранным данным и провести EDA собранных данных (в случае, если данные представляют собой текст - посчитать частотности слов, выявить наиболее частотные слова и т. п)\n",
        "\n",
        "Не забудьте сохранить полученный датасет, он вам еще пригодиться в дальнейших домашних заданиях.\n",
        "\n",
        "Критерии оценки:\n",
        "- Написан парсер, но датасет по каким-то причинам получить не удалось (например, из-за блокировок) - 4 балла\n",
        "- Написан парсер и собран датасет - 8 баллов\n",
        "- Проведен EDA собранных данных - 2 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1oXqA8L7XRvO"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from fake_useragent import UserAgent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "zm8m8fwMafxp"
      },
      "outputs": [],
      "source": [
        "def get_hrefs_from_page(page_number):\n",
        "  page_url = f'https://shazoo.ru/tags/590/anime?page={page_number}'\n",
        "  response = requests.get(page_url, headers={'User-Agent': UserAgent().chrome})\n",
        "\n",
        "  if not response.ok:\n",
        "    return []\n",
        "\n",
        "  html = response.content\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  obj = soup.findAll('h4', attrs={'class': 'leading-normal'})\n",
        "\n",
        "  hrefs_list = []\n",
        "\n",
        "  for i, el in enumerate(obj):\n",
        "    hrefs_list.append(el.a['href'])\n",
        "\n",
        "  return hrefs_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "ixGanSMIdca5"
      },
      "outputs": [],
      "source": [
        "def get_all_data_from_post(post_url):\n",
        "  response = requests.get(post_url, headers={'User-Agent': UserAgent().chrome})\n",
        "\n",
        "  html = response.content\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "  title_obj = soup.find('h1', attrs={'class': 'sm:max-w-4xl text-xl sm:text-3xl leading-tight font-bold break-words dark:text-gray-300'})\n",
        "  title = title_obj.text\n",
        "\n",
        "\n",
        "  tags_list = []\n",
        "  tags_obj = soup.findAll('ul', attrs={'class': 'flex flex-wrap gap-x-2 gap-y-1'})\n",
        "  tags_li_soup = BeautifulSoup(str(tags_obj), 'html.parser')\n",
        "  tags_li_list = tags_li_soup.findAll('li' )\n",
        "\n",
        "  for el in tags_li_list:\n",
        "    tags_list.append(el.a.text)\n",
        "\n",
        "  time_name_obj = soup.find('div', attrs={'class': 'flex items-center gap-2'})\n",
        "  time = time_name_obj.time.text\n",
        "  author = BeautifulSoup(str(time_name_obj), 'html.parser').findAll('a')[1].text\n",
        "\n",
        "  return {'title': title, 'tags': tags_list, 'time': time, 'author': author}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "4Isi9hG7iuTA"
      },
      "outputs": [],
      "source": [
        "def parse_data(n_pages=1, start_page=0):\n",
        "  df = pd.DataFrame(columns=[\"title\", \"tags\", \"time\", \"author\"])\n",
        "\n",
        "  for i in range(start_page, n_pages):\n",
        "    hrefs_list = get_hrefs_from_page(i)\n",
        "\n",
        "    for post in hrefs_list:\n",
        "      post_data = get_all_data_from_post(post)\n",
        "      df = pd.concat([df, pd.DataFrame([post_data])], ignore_index=True)\n",
        "\n",
        "      time.sleep(1)\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "4hQGxXs1rrbP"
      },
      "outputs": [],
      "source": [
        "df = parse_data(n_pages=10)\n",
        "\n",
        "df.to_csv(\"ten_pages_result.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "jup_env",
      "language": "python",
      "name": "jup_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
